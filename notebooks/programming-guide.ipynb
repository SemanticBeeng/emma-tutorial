{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emma Programming Guide\n",
    "\n",
    "## Preamble\n",
    "\n",
    "Make sure you have build the *emma-tutorial* project with\n",
    "\n",
    "```\n",
    "mvn compile\n",
    "```\n",
    "\n",
    "before running the commands below.\n",
    "\n",
    "### REPL Setup\n",
    "\n",
    "Uncomment the following code if you want to include a manually deployed `-SNAPSHOT` version of Emma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// interp.resolvers() = interp.resolvers() :+ ammonite.runtime.tools.Resolver.File(\n",
    "//   \"m2\",\n",
    "//   \"/.m2/repository\",\n",
    "//   \"/[organisation]/[module]/[revision]/[artifact]-[revision].[ext]\",\n",
    "//   m2 = true\n",
    "// )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ammonite.ops._\n",
    "import java.nio.file.Paths\n",
    "\n",
    "import $ivy.`org.emmalanguage:emma-language:0.2-rc2`\n",
    "// `provided` dependencies expected by `emma-language`\n",
    "import $ivy.`com.chuusai::shapeless:2.3.2`\n",
    "import $ivy.`org.apache.hadoop:hadoop-common:2.8.0`\n",
    "import $ivy.`org.apache.hadoop:hadoop-hdfs:2.8.0`\n",
    "// compiler plugin required for `@emma.lib` annotation\n",
    "import $plugin.$ivy.`org.scalamacros:paradise_2.11.8:2.0.1`\n",
    "\n",
    "// project classpath\n",
    "interp.load.cp(pwd / up / \"emma-tutorial-library\" / \"target\" / \"classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Data Setup\n",
    "\n",
    "The following command will setup the data used in the following examples. It will\n",
    "\n",
    "1. Download the [https://openflights.org]() data;\n",
    "1. Import \n",
    "   1. the data model (located at `data.openflights`), and \n",
    "   1. some auxiliary functions (located at `lib.openflights`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// project imports\n",
    "import org.example.emma.tutorial.data.openflights._\n",
    "import org.example.emma.tutorial.lib.openflights._\n",
    "\n",
    "// download https://openflights.org data\n",
    "downloadOpenFlightsData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic API\n",
    "\n",
    "Emma programs require the following import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.emmalanguage.api._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataBag Basics\n",
    "\n",
    "Parallel computation in Emma is represented by expressions over a generic type `DataBag[A]`. \n",
    "The type represents a distributed collection of elements of type `A` that do not have particular order and may contain duplicates.\n",
    "\n",
    "`DataBag` instances are constructed in two ways, either from a Scala `Seq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val squaresSeq = 1 to 42 map { x => (x, x * x) }\n",
    "val squaresBag = DataBag(squaresSeq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or by reading from a supported source, e.g. `CSV` or `Parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val csv = CSV(delimiter = ',')\n",
    "val airports = DataBag.readCSV[Airport](file(\"airports.dat\").toString, csv)\n",
    "val airlines = DataBag.readCSV[Airline](file(\"airlines.dat\").toString, csv)\n",
    "val routes = DataBag.readCSV[Route](file(\"routes.dat\").toString, csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, a `DataBag` can be converted to a `Seq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val squaresSeq = squaresBag.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or written to a supported file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.writeCSV(file(\"airports.copy.dat\").toString, csv)\n",
    "airlines.writeCSV(file(\"airlines.copy.dat\").toString, csv)\n",
    "routes.writeCSV(file(\"routes.copy.dat\").toString, csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declarative Dataflows\n",
    "\n",
    "In contrast to other distributed collection types such as Spark's `RDD` and Flink's `DataSet`, Emma's `DataBag` type is a proper monad. \n",
    "This means that joins and cross products in Emma can be declared using `for`-comprehension syntax in a manner akin to *Select-From-Where* expressions known from SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val flightsFromBerlin = for {\n",
    "  ap <- airports \n",
    "  if ap.city == Some(\"Berlin\")\n",
    "  rt <- routes\n",
    "  if rt.srcID == Some(ap.id)\n",
    "  al <- airlines\n",
    "  if rt.airlineID == Some(al.id)\n",
    "} yield (rt.src, rt.dst, al.name)\n",
    "\n",
    "flightsFromBerlin.sample(3).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to comprehension syntax, the `DataBag` interface offers some combinators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can `sample` `N` elemens using [reservoir sampling](https://en.wikipedia.org/wiki/Reservoir_sampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val samples = routes.map(_.src).sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine two `DataBag[A]` instances by taking their (duplicate preserving) `union`, which corresponds to `UNION ALL` clause in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val srcs = routes.map(_.src) \n",
    "val dsts = routes.map(_.dst)\n",
    "val locs = srcs union dsts\n",
    "\n",
    "locs.sample(3).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can eliminate duplicates with `distinct`, corresponds to the `DISTINCT` clause in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dupls = locs.collect().size\n",
    "val dists = locs.distinct.collect().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can extend all elements in a `DataBag` with a unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val iroutes = routes.map(_.src).zipWithIndex\n",
    "\n",
    "iroutes.sample(3).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural Recursion (Folds)\n",
    "\n",
    "In addition to the API illustrated above, the core processing primitive provided by `DataBag[A]` is a generic pattern for parallel collection processing called *structural recursion* on bags in *union representation*.\n",
    "\n",
    "To decode the meaning behind these terms, assume that instances of `DataBag[A]` are constructed exclusively by the following constructors: \n",
    "\n",
    "1. `Empty` denotes the empty bag, \n",
    "2. `Singleton(x)` denotes a singleton bag with exactly one element `x`,\n",
    "3. `Union(xs, ys)` denotes the union of two existing bags `xs` and `ys`.\n",
    "\n",
    "The assumption implies that each bag `xs` can be represented as a binary tree of constructor applications. The inner nodes of the tree are applications of the `Union` constructor, while the leafs are applications of `Empty` or `Singleton`.\n",
    "\n",
    "*Structural recursion* on bags in *union representation* works by\n",
    "\n",
    "1. systematically deconstructing the input `DataBag[A]` instance, \n",
    "2. replacing the constructors with corresponding user-defined functions, and \n",
    "3. evaluating the resulting expression.\n",
    "\n",
    "Formally, the above procedure can be specified second-order function called `fold` as follows.\n",
    "\n",
    "```scala\n",
    "def fold[B](zero: B)(init: A => B, plus: (B, B) => B): B = this match {\n",
    "  case Empty         => zero\n",
    "  case Singleton(x)  => init(x)\n",
    "  case Union(xs, ys) => plus(xs.fold(e)(s, u), ys.fold(e)(s, u))\n",
    "}\n",
    "```\n",
    "\n",
    "Note how\n",
    "\n",
    "1. `Empty` constructors are substituted by `zero` applications, \n",
    "2. `Singleton(x)` constructors are substituted by `init(x)` applications, and \n",
    "3. `Union(xs, ys)` constructors are substituted by `plus(xs, ys)` applications.\n",
    "\n",
    "A particular combination of `zero`, `init`, and `plus` function therefore defines a specific function. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dupls = locs.fold(0L)(_ => 1L, _ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is another way to compute the number of elements of `dupls`. Note that this expression will be evaluated **in parallel**, while the version we used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dupls = locs.collect().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first converts the `DataBag` **dupls** into a local `Seq[String]` and then counts the number of elements on the local machine.\n",
    "\n",
    "A convenient way to bundle a specific combination of functions passed to a `fold` is through a dedicated trait.\n",
    "\n",
    "```scala\n",
    "// defined in `org.emmalanguage.api.alg`\n",
    "trait Alg[-A, B] extends Serializable {\n",
    "  val zero: B\n",
    "  val init: A => B\n",
    "  val plus: (B, B) => B\n",
    "}\n",
    "```\n",
    "\n",
    "and overload `fold` as follows:\n",
    "\n",
    "```scala\n",
    "def fold[B](zero: B)(init: A => B, plus: (B, B) => B): B = this match {\n",
    "  case Empty         => zero\n",
    "  case Singleton(x)  => init(x)\n",
    "  case Union(xs, ys) => plus(xs.fold(e)(s, u), ys.fold(e)(s, u))\n",
    "}\n",
    "```\n",
    "\n",
    "With this extension, we can name commonly used triples as specific `Alg` instances.\n",
    "\n",
    "```scala\n",
    "object Size extends Alg[Any, Long] {\n",
    "  val zero: Long = 0\n",
    "  val init: Any => Long = const(1)\n",
    "  val plus: (Long, Long) => Long = _ + _\n",
    "}\n",
    "```\n",
    "\n",
    "and define corresponding aliases for the corresponding `fold(alg)` calls.\n",
    "\n",
    "```scala\n",
    "def size: Long = this.fold(Size)\n",
    "```\n",
    "\n",
    "The following methods `DataBag` are pre-defined in the `DataBag` trait and delegate to to a `fold` with a specific `Alg` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// cardinality tests\n",
    "locs.size      // counts the number of elements\n",
    "locs.nonEmpty  // checks if empty\n",
    "locs.isEmpty   // checks if not empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// based on an implicit `Ordering`\n",
    "locs.min       // minimum\n",
    "locs.max       // maximum\n",
    "locs.top(3)    // top-K\n",
    "locs.bottom(3) // bottom-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// arithmetic operations\n",
    "routes.map(_.stops).sum\n",
    "DataBag(1 to 5).product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// predicate testing\n",
    "routes.count(_.stops < 3)\n",
    "routes.forall(_.stops < 3)\n",
    "routes.exists(r => r.src == \"FRA\" && r.dst == \"SFO\")\n",
    "routes.find(r => r.src == \"FRA\" && r.dst == \"SFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// reducers\n",
    "DataBag(1 to 5).reduce(1)(_ * _)\n",
    "DataBag(1 to 5).reduceOption(_ * _)\n",
    "DataBag(Seq.empty[Int]).reduceOption(_ * _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Parallelisation\n",
    "\n",
    "To parallelise Emma code, you need to do two things\n",
    "\n",
    "1. Setup a parallel dataflow framework (Flink or Spark)\n",
    "2. Enclose the code fragment you want to parallelize in an `emma.onSpark` or `emma.onFlink` quote.\n",
    "\n",
    "### Parallel Dataflow Backend Setup\n",
    "\n",
    "Depending on the backend you want to use, run one of the two options below.\n",
    "\n",
    "#### Option 1: Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// `emma.onSpark` macro\n",
    "import $ivy.`org.emmalanguage:emma-spark:0.2-rc2`\n",
    "// `provided` dependencies expected by `emma-spark`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// required spark imports\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// implicit Spark environment\n",
    "implicit val backend = SparkSession.builder()\n",
    "    .appName(\"Emma Programming Guide\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.sql.warehouse.dir\", Paths.get(sys.props(\"java.io.tmpdir\"), \"spark-warehouse\").toUri.toString)\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Flink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.emmalanguage:emma-flink:0.2-rc2`\n",
    "import $ivy.`org.apache.flink::flink-scala:1.2.1`\n",
    "import $ivy.`org.apache.flink::flink-clients:1.2.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.flink.api.scala.ExecutionEnvironment\n",
    "implicit val backend = ExecutionEnvironment.getExecutionEnvironment\n",
    "backend.getConfig.disableSysoutLogging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Include Flink target classes in the classpath\n",
    "val codegenDir = org.emmalanguage.compiler.RuntimeCompiler.default.instance.codeGenDir\n",
    "interp.load.cp(ammonite.ops.Path(codegenDir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quotations\n",
    "\n",
    "Once you have an implicit `SparkSession` or `ExecutionEnvironment` instance in scope, you can wrap code that you want to run in parallel in an `emma.onSpark` or `emma.onFlink` quote. \n",
    "\n",
    "The wrapped is optimized holistically by the Emma compiler, and `DataBag` expressions are offloaded to the corresponding parallel exeuction engine.\n",
    "\n",
    "For convenience, we alias of the quote used in the examples below to `emma.onSpark`. Change the alias to `emma.onFlink` and re-evaluate the following code snippets to parallelize them on Flink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val quote = emma.onSpark // or emma.onFlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// override the `airports`, `routes`, and `airlines` members\n",
    "// with the `def`-based versions from `data.openflights` module\n",
    "import org.example.emma.tutorial.data.openflights._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// evaluates as map and filter\n",
    "val berlinAirports = quote {\n",
    "  for {\n",
    "    a <- airports\n",
    "    if a.latitude > 52.3\n",
    "    if a.latitude < 52.6\n",
    "    if a.longitude > 13.2\n",
    "    if a.longitude < 13.7\n",
    "  } yield Location(\n",
    "    a.name,\n",
    "    a.latitude,\n",
    "    a.longitude)\n",
    "}\n",
    "\n",
    "berlinAirports.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// evaluates as cascasde of joins\n",
    "val rs = quote {\n",
    "  for {\n",
    "    ap <- airports\n",
    "    rt <- routes\n",
    "    al <- airlines\n",
    "    if rt.srcID == Some(ap.id)\n",
    "    if rt.airlineID == Some(al.id)\n",
    "  } yield (al.name, ap.country)\n",
    "}\n",
    "\n",
    "rs.sample(3).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// evaluates using partial aggregates (reduce / reduceByKey)\n",
    "val aggs = quote {\n",
    "  for {\n",
    "    Group(k, g) <- routes.groupBy(_.src)\n",
    "  } yield {\n",
    "    val x = g.count(_.airline == \"AB\")\n",
    "    val y = g.count(_.airline == \"LH\")\n",
    "    k -> (x, y)\n",
    "  }\n",
    "}\n",
    "\n",
    "aggs.withFilter({ case (k, (x, y)) => x > 0 && y > 0 }).sample(3).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Modularity\n",
    "\n",
    "To build domain-specific libraries based on Emma, enclose your function definitions in a top-level object and annotate this object with the `@emma.lib` annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@emma.lib\n",
    "object hubs {\n",
    "  def apply(M: Int) = {\n",
    "    val rs = for {\n",
    "      Group(k, g) <- ({\n",
    "        routes.map(_.src)\n",
    "      } union {\n",
    "        routes.map(_.dst)\n",
    "      }).groupBy(x => x)\n",
    "      if g.size < M\n",
    "    } yield k\n",
    "\n",
    "    rs.collect().toSet\n",
    "  }\n",
    "}\n",
    "\n",
    "@emma.lib\n",
    "object reachable {\n",
    "  def apply(N: Int)(hubs: Set[String]) = {\n",
    "     val hubroutes = routes\n",
    "       .withFilter(r => hubs(r.src) && hubs(r.dst))\n",
    "\n",
    "     var paths = hubroutes\n",
    "       .map(r => Path(r.src, r.dst))\n",
    "     for (_ <- 0 until N) {\n",
    "       val delta = for {\n",
    "         r <- hubroutes; p <- paths if r.dst == p.src\n",
    "       } yield Path(r.src, p.dst)\n",
    "       paths = (paths union delta).distinct\n",
    "     }\n",
    "\n",
    "     paths\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// override the `hubs` and `reachable` definitions from above\n",
    "// with the identical implementations in the `lib.openflights`\n",
    "// module in order to fix a bug in the type handling\n",
    "import org.example.emma.tutorial.lib.openflights._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// evaluates as Spark RDD reduceByKey\n",
    "val rs = quote {\n",
    "  reachable(2)(hubs(50))\n",
    "}\n",
    "\n",
    "rs.sample(3).foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
